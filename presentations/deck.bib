@online{course,
  author = {Jure Leskovec, Stanford University},
  title = {CS224W: Machine Learning with Graphs},
  url = {https://snap.stanford.edu/class/cs224w-2021/},
  year = {2021}
}

@article{paper,
  author={Hu, Linmei and Zhang, Mengmei and Li, Shaohua and Shi, Jinghan and Shi, Chuan and Yang, Cheng and Liu, Zhiyuan},
  title={Text-Graph Enhanced Knowledge Graph Representation Learning},
  journal={Frontiers in Artificial Intelligence},
  volume={4},
  year={2021},      
  url={https://www.frontiersin.org/articles/10.3389/frai.2021.697856},       
  doi={10.3389/frai.2021.697856},
  issn={2624-8212},   
  abstract={Knowledge Graphs (KGs) such as Freebase and YAGO have been widely adopted in a variety of NLP tasks. Representation learning of Knowledge Graphs (KGs) aims to map entities and relationships into a continuous low-dimensional vector space. Conventional KG embedding methods (such as TransE and ConvE) utilize only KG triplets and thus suffer from structure sparsity. Some recent works address this issue by incorporating auxiliary texts of entities, typically entity descriptions. However, these methods usually focus only on local consecutive word sequences, but seldom explicitly use global word co-occurrence information in a corpus. In this paper, we propose to model the whole auxiliary text corpus with a graph and present an end-to-end text-graph enhanced KG embedding model, named Teger. Specifically, we model the auxiliary texts with a heterogeneous entity-word graph (called text-graph), which entails both local and global semantic relationships among entities and words. We then apply graph convolutional networks to learn informative entity embeddings that aggregate high-order neighborhood information. These embeddings are further integrated with the KG triplet embeddings via a gating mechanism, thus enriching the KG representations and alleviating the inherent structure sparsity. Experiments on benchmark datasets show that our method significantly outperforms several state-of-the-art methods.}
}